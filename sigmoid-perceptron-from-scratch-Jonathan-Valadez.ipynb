{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Tarea #3: Implementación de un perceptrón clasificador para clasificación binaria**\n","### **Desde cero**\n","\n","# Universidad de Monterrey\n","\n","* Estudiante: Jonathan A. Valadez Saldaña\n","\n","* Materia: Inteligencia Artificial II\n","\n","* Profesor:  Andrés Hernández  Gutiérrez \n","\n","19/Septiembre/2024 - San Pedro Garza García, N.L."]},{"cell_type":"markdown","metadata":{},"source":["Siempre cuando se empieza un código en python es importante importar las librerías que vayamos a ocupar para nuestro programa, en este caso usamos las siguientes librerías que cuentan con las siguientes características:\n","* **Fetchucimlrepo_ucirepo  > fetch_ucirepo:** Paquete para importar fácilmente datasets desde el repositorio de UC Irvine Machine Learning  a scripts y cuadernos.\n","* **Pandas:** Se usa para leer y guardar datos (ingeniería de datos)\n","* **Plotly.express:** Es una herramienta versátil y  útil para crear cualquier tipo de gráfica como: lineal, de barras, circular, de burbuja, histogramas, diagrama de cajas, barras de error, diagramas de dispersión, gráficas polares, mapas de color, etc.\n","* **Seaborn**: Nos permite desplegar gráficas estadísticas\n","* **Matplotlib:** Visualización  de gráficas de datos\n","* **Numpy:** Nos ayuda a realizar operaciones matemáticas\n","* **Tensorflow:** Nos permite el desarrollo de redes neuronales \n","* **Sklearn.model_selection > train_test_split:** Nos permite dividir los datos de entrenamiento y los de prueba\n","* **Sklearn.preprocessing >StandardScaler:** Nos permite escalar los valores de nuestros datos, esto para tenerlos en un mismo rango de valores y poder hacer una mejor interpretación de los mismos\n","* **Sklearn.metrics > ConfusionMatrixDispla:** Nos permite contabilizar y visualizar los Verdaderos Positivos (TP), Verdaderos Negativos (TN), Falsos Positivos (FP) y los Falsos Negativos (FN)"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37019,"status":"ok","timestamp":1726229336734,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"REyK8cG6Qisu","outputId":"f2c6ce0d-0539-4e44-ca3d-274be2288b27"},"outputs":[],"source":["#!pip install ucimlrepo\n","\n","from ucimlrepo import fetch_ucirepo\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"markdown","metadata":{},"source":["Descargamos el dataset del sitio que se nos indica y dirige arriba, y posteriormente accedemos a la información de nuestro interés, en nuestro caso que estamos estudiando cuando una habitación fue ocupada (occupancy_detection) o no dependiendo de la humedad, temperatura y otras variables dentro de la misma, entonces usamos el id de esta información, que viene siendo 357, y una vez hecho esto lo guardamos como 'data'"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2040,"status":"ok","timestamp":1726229340831,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"VR0P_ZhYQisw"},"outputs":[],"source":["# Download `occupancy-detection` dataset\n","# using the id=357\n","occupancy_detection = fetch_ucirepo(id=357)\n","\n","# Build the feature and the target data sets\n","data = occupancy_detection.data.original"]},{"cell_type":"markdown","metadata":{},"source":["Aquí lo que hacemos es una limpieza de los datos, como quitando las columnas 'date' y 'id', y para quitar datos se usa la función 'drop' y para indicar si es para una columna de cierta categoría o variable se coloca 'axis=1', ya que si usamos 0 estaríamos eliminando una fila. \n","En la segunda línea tenemos que seleccionamos todas las columnas de nuestro dataframe (df.columns) y le aplicamos a dichas columnas 'pd.to_numeric', esto lo que hará es que convierte los valores numéricos a un solo tipo de dato numérico, es decir, valores flotantes o enteros, y el parámetro 'errors' puede tomar 3 opciones: \n","* 'raise': que genera una excepción a un análisis inválido (textos, palabras, o letras)\n","* 'coerce': que despliega 'NaN' cuando se hace un análisis inválido\n","* 'ignore': el  análisis inválido se devolverá en la entrada\n","Y por último eliminamos las filas que contentan esos valores inválidos (NaN) de las columnas analizadas, usando la función 'dropna()' que también se puede leer como 'eliminar espacios vacíos'."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":666,"status":"ok","timestamp":1726229343639,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"rew6AD7bQisw"},"outputs":[],"source":["df = data.drop(['date', 'id'], axis=1)\n","df[df.columns] = df[df.columns].apply(pd.to_numeric, errors='coerce')\n","df = df.dropna()"]},{"cell_type":"markdown","metadata":{},"source":["Imprimimos nuestro dataframe (df) para tener una primera vista de lo que contiene, y corroborar que están las variables o datos que necesitamos"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"gfl5mAE1Qisw","outputId":"884f7299-2296-4c7a-ed22-05f11cff96f1"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Temperature</th>\n","      <th>Humidity</th>\n","      <th>Light</th>\n","      <th>CO2</th>\n","      <th>HumidityRatio</th>\n","      <th>Occupancy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>23.180</td>\n","      <td>27.2720</td>\n","      <td>426.00</td>\n","      <td>721.25</td>\n","      <td>0.004793</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>23.150</td>\n","      <td>27.2675</td>\n","      <td>429.50</td>\n","      <td>714.00</td>\n","      <td>0.004783</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>23.150</td>\n","      <td>27.2450</td>\n","      <td>426.00</td>\n","      <td>713.50</td>\n","      <td>0.004779</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>23.150</td>\n","      <td>27.2000</td>\n","      <td>426.00</td>\n","      <td>708.25</td>\n","      <td>0.004772</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>23.100</td>\n","      <td>27.2000</td>\n","      <td>426.00</td>\n","      <td>704.50</td>\n","      <td>0.004757</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>20557</th>\n","      <td>20.815</td>\n","      <td>27.7175</td>\n","      <td>429.75</td>\n","      <td>1505.25</td>\n","      <td>0.004213</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>20558</th>\n","      <td>20.865</td>\n","      <td>27.7450</td>\n","      <td>423.50</td>\n","      <td>1514.50</td>\n","      <td>0.004230</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>20559</th>\n","      <td>20.890</td>\n","      <td>27.7450</td>\n","      <td>423.50</td>\n","      <td>1521.50</td>\n","      <td>0.004237</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>20560</th>\n","      <td>20.890</td>\n","      <td>28.0225</td>\n","      <td>418.75</td>\n","      <td>1632.00</td>\n","      <td>0.004279</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>20561</th>\n","      <td>21.000</td>\n","      <td>28.1000</td>\n","      <td>409.00</td>\n","      <td>1864.00</td>\n","      <td>0.004321</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>20560 rows × 6 columns</p>\n","</div>"],"text/plain":["       Temperature  Humidity   Light      CO2  HumidityRatio  Occupancy\n","0           23.180   27.2720  426.00   721.25       0.004793        1.0\n","1           23.150   27.2675  429.50   714.00       0.004783        1.0\n","2           23.150   27.2450  426.00   713.50       0.004779        1.0\n","3           23.150   27.2000  426.00   708.25       0.004772        1.0\n","4           23.100   27.2000  426.00   704.50       0.004757        1.0\n","...            ...       ...     ...      ...            ...        ...\n","20557       20.815   27.7175  429.75  1505.25       0.004213        1.0\n","20558       20.865   27.7450  423.50  1514.50       0.004230        1.0\n","20559       20.890   27.7450  423.50  1521.50       0.004237        1.0\n","20560       20.890   28.0225  418.75  1632.00       0.004279        1.0\n","20561       21.000   28.1000  409.00  1864.00       0.004321        1.0\n","\n","[20560 rows x 6 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"markdown","metadata":{},"source":["Visualizamos  la descripción estadística de nuestro df usando 'describe'. Aquí podremos ver estadística básica como las medias, mediana, modas, mínimos, máximos y cuartiles de nuestras variables independientes y dependiente"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"elapsed":562,"status":"ok","timestamp":1726229347094,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"ELCVREAKQisw","outputId":"cce4a981-4706-406f-f635-fdfd47f54eeb"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Temperature</th>\n","      <th>Humidity</th>\n","      <th>Light</th>\n","      <th>CO2</th>\n","      <th>HumidityRatio</th>\n","      <th>Occupancy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>20560.000000</td>\n","      <td>20560.000000</td>\n","      <td>20560.000000</td>\n","      <td>20560.000000</td>\n","      <td>20560.000000</td>\n","      <td>20560.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>20.906212</td>\n","      <td>27.655925</td>\n","      <td>130.756622</td>\n","      <td>690.553276</td>\n","      <td>0.004228</td>\n","      <td>0.231031</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>1.055315</td>\n","      <td>4.982154</td>\n","      <td>210.430875</td>\n","      <td>311.201281</td>\n","      <td>0.000768</td>\n","      <td>0.421503</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>19.000000</td>\n","      <td>16.745000</td>\n","      <td>0.000000</td>\n","      <td>412.750000</td>\n","      <td>0.002674</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>20.200000</td>\n","      <td>24.500000</td>\n","      <td>0.000000</td>\n","      <td>460.000000</td>\n","      <td>0.003719</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>20.700000</td>\n","      <td>27.290000</td>\n","      <td>0.000000</td>\n","      <td>565.416667</td>\n","      <td>0.004292</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>21.525000</td>\n","      <td>31.290000</td>\n","      <td>301.000000</td>\n","      <td>804.666667</td>\n","      <td>0.004832</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>24.408333</td>\n","      <td>39.500000</td>\n","      <td>1697.250000</td>\n","      <td>2076.500000</td>\n","      <td>0.006476</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        Temperature      Humidity         Light           CO2  HumidityRatio  \\\n","count  20560.000000  20560.000000  20560.000000  20560.000000   20560.000000   \n","mean      20.906212     27.655925    130.756622    690.553276       0.004228   \n","std        1.055315      4.982154    210.430875    311.201281       0.000768   \n","min       19.000000     16.745000      0.000000    412.750000       0.002674   \n","25%       20.200000     24.500000      0.000000    460.000000       0.003719   \n","50%       20.700000     27.290000      0.000000    565.416667       0.004292   \n","75%       21.525000     31.290000    301.000000    804.666667       0.004832   \n","max       24.408333     39.500000   1697.250000   2076.500000       0.006476   \n","\n","          Occupancy  \n","count  20560.000000  \n","mean       0.231031  \n","std        0.421503  \n","min        0.000000  \n","25%        0.000000  \n","50%        0.000000  \n","75%        0.000000  \n","max        1.000000  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df.describe()"]},{"cell_type":"markdown","metadata":{},"source":["Usamos una semilla para poder reproducir nuestro proyecto las veces que sean, sin que estén cambiando constantemente los datos, siendo en este caso la semilla 4,500"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":381,"status":"ok","timestamp":1726229349123,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"_qmL27_KQisw"},"outputs":[],"source":["tf.random.set_seed(4500)"]},{"cell_type":"markdown","metadata":{},"source":["Construimos nuestras variables independientes (X) y nuestra variable dependiente (Y), el primero como hemos visto, ya tenemos limpio nuestro df, solo queda quitar la columna de la variable 'Occupancy', y ya en la siguiente línea volvemos a llamar nuestro df, pero seleccionando únicamente a 'Occupancy' para nuestra variable de salida. "]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":417,"status":"ok","timestamp":1726229350763,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"4sIFY_kAQisw"},"outputs":[],"source":["X = df.drop([\"Occupancy\"], axis=1)\n","y = df['Occupancy']"]},{"cell_type":"markdown","metadata":{},"source":["Dividimos nuestras variables X y Y en variables de entrenamiento y de prueba, donde esta última se utilizarán el 20% de los datos de X y Y, y está división tendrá una semilla de 42,  y  'shuffle=True' es para que se realice dicha mezcla (aleatoria) de los datos."]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":305,"status":"ok","timestamp":1726229352060,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"6yPyLEblQisw"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)"]},{"cell_type":"markdown","metadata":{},"source":["Escalamos nuestros datos X de entrenamiento y de prueba usando 'StandardScaler', y lo aplicamos usando las funciones 'fit_transform' y 'transform', una ajusta y aplica la transformación a los datos, y la otra ya no ocupa ajustarla, ya solo aplica la transformación a los datos."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1726229353635,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"O7J3VULnQisx"},"outputs":[],"source":["scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":332,"status":"ok","timestamp":1726229356530,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"sxdZAxWxQisx"},"outputs":[],"source":["from scipy.special import expit     # De la librería \"scipy.special\" importamos la función \"expit\", que viene siendo la función sigmodea\n","\n","class NeuronModel():        # Creamos la clase \"NeuronModel\", la cual va contener todos los parámetros y métodos necesarios de nuestro modelo, \n","                                                            # y de esa forma poder llamarlo desde cualquier parte del código\n","\n","    \"\"\"\n","     Comenzamos con la función \"__init__\" , que como su nombre indica, va ser quién inicialice la primera parte de la configuración de nuestro modelo.\n","\n","     Los parámetros son los siguientes:\n","    - self: Se usa para acceder a estancias u objetos actuales en los métodos de la clase\n","    - X: Es nuestro arreglo (matriz) del dataset que contiene  las variables independientes con las que vamos a trabajar.\n","    - y: Es nuestra salida proveniente de nuestro dataset.\n","    - learning_rate: Es nuestro taza de aprendizaje, la cual se ajusta moviendo el punto decimal hacía la izquierda o derecha, agregando 0's  \n","        en los espacios recorridos. El valor default que se toma es de 0.01. \n","    - error_threshold: Es una tolerancia a nuestro valor objetivo que queremos que maneje nuestro modelo, además de ser un modo de paro \n","        para las iteraciones de este. Esta tolerancia depende del criterio que maneje el usuario.\n","    \"\"\"\n","    \n","    def __init__(self, X, y, learning_rate=0.01, error_threshold=0.001) -> None:\n","\n","        # El uso de \"assert\" se usa para testear nuestro código o software para asegurarnos de que la expresión testeada  sea correcta, en caso contrarío \n","        # se lanzará una excepción o un mensaje\n","\n","        assert X.size != 0, \"X cannot be empty\"     # Confirmamos que el tamaño del arreglo \"X\" sea diferentes de 0, de lo contrario se arroja el mensaje \"X cannot be empty\"\n","        assert y.size != 0, \"y cannot be empty\"     # Confirmamos que el tamaño del arreglo '\"y\" sea diferentes de 0, de lo contrario se arroja el mensaje \"X cannot be empty\"\n","        assert learning_rate > 0, \"learning rate must be positive\"      # Confirmamos que el valor de la variable \"learning_rate\" sea mayor a 0, de lo contrario se arroja el \n","                                                                                                                                                        # mensaje \"learning rate must be positive\"\n","        \"\"\"\n","        \"isinstance\" se usa para verificar si una variable u objeto  pertecene a un tipo de clase de dato (int, string, char, etc), en este caso se verifican si las \n","        estancias \"x\" y \"y\" no son del tipo \"np.ndarray\", en caso de que se cumpla esta condición (que no pertenezcan a dicha estancia), con el uso de \"to_numpy\"\n","        transforma dichas estancias a una de tipo numpy.ndarray.\n","\n","        En la estancia \"y\" además de transformar la clase de dato, también reacomodamos el arreglo, usando \"reshape\" donde -1 se usa cuando no sabemos un número \n","        exacto de filas, por lo que la misma función hace esta tarea de manera automática, y el 1 para decir que va ser de una sola columna.\n","        \"\"\"\n","\n","        if not isinstance(X, np.ndarray):   # Si \"X\" no es de tipo np.ndarray\n","            X = X.to_numpy()                                    # entoncec conviertelo a uno de ese tipo\n","\n","        if not isinstance(y, np.ndarray):       # # Si \"y\" no es de tipo np.ndarray\n","            y = y.to_numpy().reshape(-1,1)  # entoncec conviertelo a uno de ese tipo, además reacomoda esta estancia en una columna\n","\n","        self.X = X      # Asignamos el valor de \"X\"  al atributo \"self.X  de la estancia\"\n","        self.y = y      # Asignamos el valor de \"y\"  al atributo \"self.y  de la estancia\"\n","        self.learning_rate = learning_rate       # Asignamos el valor de \"learning_rate\"  al atributo \"self.learning_rate  de la estancia\"\n","        self.w = np.zeros((X.shape[1], 1))          #  Asignamos al atributo \"self.w\" una matriz de ceros, con un número de filas igual al \n","                                                                                                    # tamaño de X.shape[1] (número de columnas de X),  por 1 columna\n","        self.b = np.zeros((1, 1))                                 #  Asignamos al atributo \"self.b\" una matriz de ceros, de 1 fila y 1 columna\n","        self.N = X.shape[0]     # Asignamos al atributo  \"self.N\" el valor del tamaño de X.shape[0] (número de filas de X)\n","        self.J_iter = list()       # Asignamos al atributo \"self.J_iter\" una lista vacía\n","        self.stopping_tolerance = error_threshold       # Asignamos al atributo \"self.stopping_tolerance\" el valor de tolerancia del error\n","\n","    # Definimos la función o método \"compute_logistic\", que recibe como parámetros  los atributos: self, w, b, X\n","    def compute_logistic(self, w, b, X):\n","        return expit(X@w + b)   # Calculamos  la función sigmoidea del producto matricial entre X y w, sumamos el  sesgo (b) y devolvemos el resultado\n","\n","    # Definimos la función o método \"compute_cost\", que recibe como parámetros  los atributos: self, y_hat, y, N\n","    def compute_cost(self, y_hat, y, N):\n","        L = y*np.log(y_hat) + (1-y)*np.log(1-y_hat)     # Calculamos la pérdida de la entropía cruzada\n","        J = -L.sum() / N                                                                            # Calculamos la función de costo (promedio de la pérdida)\n","        return J                                                                                            # Devolvemos el valor de la función de costo\n","    \n","    # Definimos la función o método \"forward_propagation\", que recibe como parámetros  los atributos:  self, w, b, X\n","    def forward_propagation(self, w, b, X):\n","        a = self.compute_logistic(w, b, X)      # Llamamos la función antes mencionada \"compute_logistic\" y calculamos la \n","                                                                                           # función sigmoidea con los atributos dados\n","        return a                                                               # Guardamos el resultado en la variable \"a\" y la devolvemos\n","\n","    # Definimos la función o método \"backward_propagation\", que recibe como parámetros  los atributos:  self, y_hat, y, w, X \n","    def backward_propagation(self, y_hat, y, w, X):\n","        N = X.shape[0]                                                                                                  # Asignamos N el valor de X.shape[0] (número de filas de X)\n","        gradient_w = np.multiply(y_hat - y, X).sum(axis=0)/ N     # Restamos las predicciones de \"y_hat\" con los valores verdaderos de \"y\",\n","                                                                                                                                            # a esto lo multiplicamos por X y lo sumamos a lo largo de las filas (axis=0), \n","                                                                                                                                            # en otras palabras, suma los valores en cada columna, una fila con varias\n","                                                                                                                                            # sumas de cada característica. Y finalmente dividimos todo por N. \n","        gradient_w = gradient_w.reshape(w.shape)                           # Actualizamos el gradiente de los pesos, reacomodando el arreglo de este \n","                                                                                                                                            # según el tamaño que tiene \"w\", asegurando que tenga la misma dimensión que \"w\".\n","\n","        gradient_b = (y_hat - y).sum() / N                                                  # Restamos las predicciones de \"y_hat\" con los valores verdaderos de \"y\",\n","                                                                                                                                            # sumamos todos los resultados y lo dividimos por N\n","        gradient_b = gradient_b.reshape(-1,1)                                    # Reorganizamos el gradiente de sesgo (b), de filas desconocidas (-1)  por 1 columna\n","        return gradient_w, gradient_b                                                           # Devolvemos los resultados de ambos gradientes\n","    \n","    # Definimos la función o método \"update_parameters\", que recibe como parámetros  los atributos:  self, param, gradient\n","    def update_parameters(self, param, gradient):\n","        return np.subtract(param, np.multiply(self.learning_rate, gradient))    # Multiplicamos la taza de aprendizaje por el gradiente, el resultado \n","                                                                                                                                                                                    # viene siendo un ajsute , el cual va restar al parámetro actual, lo que\n","                                                                                                                                                                                    # nos va generar como resultado un nuevo parámetro (actualizado) \n","\n","    # Definimos la función o método \"train\", que recibe como parámetros  los atributos:  self, verbose=0    \n","    # (verbose=0 se usa para mostrar un tipo de métricas en el progreso de las iteraciones)                                                                                                                                                                       \n","    def train(self, verbose=0):\n","\n","        y_hat = self.forward_propagation(self.w, self.b, self.X)    # Calculamos \"y_hat\" llamando a la función \"forward_propagation\" con los valores\n","                                                                                                                                                  # iniciales (siendo ceros) de \"self.w\", \"self.b\" y \"self.X\", antes de entrar al ciclo \"while, \n","                                                                                                                                                  # que servirá para calcular el primer valor de la función de costo\n","        J_prev = self.compute_cost(y_hat, self.y, self.N)                   # Calculamos \"J_prev\" llamando la función \"compute_cost\"\n","        J, current_percentage_error = 0, 100                                            # Inicializamos \"J\" (función de costo) con un valor de 0, y \"current_percentage_error\" \n","                                                                                                                                                  # (porcentaje de error actual) con un valor de 100%, ya que apenas vamos a entrenar\n","                                                                                                                                                  # el modelo y es lógico que no sepa nada y tenga un valor de error del 100%.\n","        w, b = self.w, self.b                                                                                               # Asignamos los valores de \"self.w\" y \"self.b\" a las variables \"w\" y \"b\", de esta forma \n","                                                                                                                                                  # nos ayudarán a qu e se actualicen los pesos y los sesgos dentro del entrenamiento\n","                                                                                                                                                  # pero solo dentro de las variables \"w\" y \"b\"\n","\n","        while current_percentage_error > self.stopping_tolerance:       # Mientras el % del error actual sea mayor a la tolerancia del mismo, se seguirá \n","                                                                                                                                                                  # actualizando la función de costo (se guarda en \"J_prev\")\n","            J_prev = J\n","\n","            y_hat = self.forward_propagation(w, b, self.X)                                      # Volvemos a hacer el cálculo de \"y_hat\" con una primera actualización de \n","                                                                                                                                                                  # los pesos y sesgos \n","\n","            gradient_w, gradient_b = self.backward_propagation(y_hat,     # Calculamos los gradientes de los pesos (w) y sesgos (b) usando la función \n","                                                                                                                                                                    # \"backward_propagation\", que recibe sus respectivos parámetros\n","                                                               self.y,\n","                                                               w,\n","                                                               self.X)\n","\n","            w = self.update_parameters(w, gradient_w)       # Usamos la función \"update_parameters\" para actualizar los pesos con el gradiente del mismo\n","                                                                                                                            # que fueron  previamente calculados en la función anterior\n","            b = self.update_parameters(b, gradient_b)       # Usamos la función \"update_parameters\" para actualizar los sesgos  con el gradiente del mismo\n","                                                                                                                            # que fueron  previamente calculados en la función \"backward_propagation\"\n","\n","            y_hat = self.forward_propagation(w, b, self.X)      #  Volvemos a calcular \"y_hat\" con los pesos y sesgos ajustados\n","            J = self.compute_cost(y_hat, self.y, self.N)               # Obtenemos nuestra nueva función de costos (la actual en esta iteración)\n","            self.J_iter.append(J)                                                                  # La almacenamos en nuestra lista de valores de J \n","\n","            current_percentage_error = np.abs((J - J_prev)/J)*100       # Calculamos el error porcentual y lo actualizamos en   \n","                                                                                                                                                                    # nuestra variable current_percentage_error\n","\n","            if verbose:     # Si \"verbose\" es verdadero o posee algún valor diferente de 0 o None, se ejecutan las intrucciones dentro del \"if\"\n","                print(f\"J_previous: {J_prev:8.6f} \\t\"       # Imprimimos el penúltimo valor de la función de costo \"J_previous\" \n","                      f\"J_current: {J:8.6f} \\t\"                                # Imprimimos el último valor de la función de costo \"J_current\" (el valor actual de J)\n","                      f\"Error: {current_percentage_error:8.6f}\")        # Imprimimos el error porcentual actual\n","                \"\"\"\n","                 NOTA: Todos estos valores serán impresos usando el formato \"8.6f\". Esto quiere decir que el número sera impreso únicamente\n","                con 8 carácteres, incluyento el punto decimal. El 6 se refiere a que después del punto, habrá 6 números decimales a la derecha \n","                de este, y por último la \"f\" describe el tipo de dato que se desplegara, siendo de tipo flotante\n","                \"\"\"\n","        # Una vez entrenado el modelo y hallado los valores óptimos de \"w\" y \"b\", estos serán guardados en sus respectivos \"self\"\n","        self.w = w      # Guardamos el valor óptimo de \"w\" en el atributo \"self.w\" de la clase (NeuronModel)\n","        self.b = b      # Guardamos el valor óptimo de \"b\" en  el atributo \"self.b\" de la clase (NeuronModel)\n","\n","    # Definimos la función o método \"predict\", que recibe como parámetros  los atributos: self.w, self.b, X\n","    def predict(self, X):\n","        a = self.forward_propagation(self.w, self.b, X)     # Llamamos a nuestra función de activación usando el método \"forward_propagation\"\n","        return a                                                                                                # y devolvemos el valor de esta \n","\n","    # Definimos la función o método \"plot_cost_function\", que recibe como parámetro  el atributo: self\n","    def plot_cost_function(self):       \n","        plt.plot(self.J_iter)                                   # Graficamos la lista de valores de cada función de costo generada en cada iteración\n","        plt.xlabel('Iteration')                                # En nuestro eje \"x\" imprimimos el texto \"Iteration\"\n","        plt.ylabel(r\"$J_{\\mathbf{w}}$\")     # En nuestro eje \"y\" imprimimos el texto matemático de nuestra función de costo\n","\n","    # Definimos la función o método \"evaluate\", que recibe como parámetros  los atributos: self, y_hat, y\n","    def evaluate(self, y_hat, y):\n","\n","        if not isinstance(y, np.ndarray):          # Si \"y\" no es de tipo \"np.ndarray\" dentro de la estancia, entonces \n","            y = y.to_numpy().reshape(-1,1)      # lo convertimos a ese tipo de dato, y lo reacomodamos por una  \n","                                                                                                # cantidad desconocida de filas (-1), por 1 columna\n","\n","        y_hat[y_hat >= 0.5] = 1         # Si la predicción de \"y_hat\" es igual o mayor a 0.5, automáticamente eso es un 1\n","        y_hat[y_hat < 0.5] = 0            # Si la predicción de \"y_hat\" es menor a 0.5, automáticamente eso es un 0\n","\n","        err = np.where((np.abs(y - y_hat) > 0) == True)[0]      # Guardamos los índices donde no coidican \"y_hat\" y \"y\"\n","        \"\"\"\n","        Primero calculamos el absoluto de las diferencias entre \"y\" y \"y_hat\".\n","        Luego comparamos dichas diferencias absolutas si son mayores a 0, en caso de que lo sean se, convierten \n","        en \"True\" aquellas diferencias que resulten 1, y en \"False\" aquellas que realmente den como diferencia un 0.\n","        Ahora usando \"np.where\", obtenemos una tupla, donde el primer elemento es un arreglo con los índices donde \n","        la condición sea \"True\" (y el 2do elemento queda vacío). Y por último el \"[0]\" sirve para seleccionar el 1er \n","        elemento de dicha tupla. \n","\n","        De no hacer esto último quedaría algo así: (array([i]),)\n","        Colocando el [0], quedaría algo así: array([i])\n","        \"\"\"\n","        return 1-err.shape[0]/y.shape[0]        # Devolvemos la precisión del modelo \n","        \"\"\"\n","        NOTA: shape devuelve una tupla de valores, siendo el 1er elemento el número de las filas y el 2do el de las columnas\n","        por lo que que si colocamos \"[0]\" estamos tomando únicamente las filas, ya son las que nos interesan (\"err\" contiene los índices \n","        donde están estos errores y con \"shape[0]\"\" solo estamos contabilizándolos). Ahora estamos obteniendo el coeficiente entre\n","        el número total de errores y el número total de datos, y al resultado se lo restamos a 1, dándonos  un valor entre 0 y 1\n","        \"\"\"\n","    # Definimos la función o método \"get_parameters\", que recibe como parámetro  el atributo: self\n","    def get_parameters(self):\n","        return np.vstack((self.w, self.b))      # Usamos \"np.vstack\" para apilar los arrelgos de \"w\" y \"b\" de manera vertical y en el orden\n","                                                                                                # en el que son recibidos"]},{"cell_type":"markdown","metadata":{},"source":["Guardamos nuestra clase en la variable \"model\", al mismo tiempo que nuestra clase (NeuronModel) recibe como parámetros los datos de \"X_train\" que son guardados en la variable \"X\", \"y_train\" que son guardados en la variable \"y\", \"learning_rate = 1\" (el cual puede ser ajsutado según el criterio que maneje el usuario), y \"error_threshold = 0.01\" (que también depende de la tolerancia al error que le quiera dar al usuario)\n","\n","Esta variable nos sirve para ejecutar todo el modelo desarrollado "]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11330,"status":"ok","timestamp":1726229552006,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"NCoQ267PQisx","outputId":"a0402f07-057c-4857-f4c2-12bb0ffd7cf8"},"outputs":[{"name":"stdout","output_type":"stream","text":["J_previous: 0.000000 \tJ_current: 0.296154 \tError: 100.000000\n","J_previous: 0.296154 \tJ_current: 0.139829 \tError: 111.796898\n","J_previous: 0.139829 \tJ_current: 0.086450 \tError: 61.744942\n","J_previous: 0.086450 \tJ_current: 0.069500 \tError: 24.388726\n","J_previous: 0.069500 \tJ_current: 0.064312 \tError: 8.067442\n","J_previous: 0.064312 \tJ_current: 0.061854 \tError: 3.973763\n","J_previous: 0.061854 \tJ_current: 0.060355 \tError: 2.483735\n","J_previous: 0.060355 \tJ_current: 0.059337 \tError: 1.714582\n","J_previous: 0.059337 \tJ_current: 0.058605 \tError: 1.250183\n","J_previous: 0.058605 \tJ_current: 0.058057 \tError: 0.944343\n","J_previous: 0.058057 \tJ_current: 0.057635 \tError: 0.731551\n","J_previous: 0.057635 \tJ_current: 0.057304 \tError: 0.577926\n","J_previous: 0.057304 \tJ_current: 0.057039 \tError: 0.464042\n","J_previous: 0.057039 \tJ_current: 0.056824 \tError: 0.377898\n","J_previous: 0.056824 \tJ_current: 0.056648 \tError: 0.311663\n","J_previous: 0.056648 \tJ_current: 0.056501 \tError: 0.260028\n","J_previous: 0.056501 \tJ_current: 0.056377 \tError: 0.219284\n","J_previous: 0.056377 \tJ_current: 0.056272 \tError: 0.186781\n","J_previous: 0.056272 \tJ_current: 0.056182 \tError: 0.160593\n","J_previous: 0.056182 \tJ_current: 0.056104 \tError: 0.139293\n","J_previous: 0.056104 \tJ_current: 0.056035 \tError: 0.121815\n","J_previous: 0.056035 \tJ_current: 0.055975 \tError: 0.107352\n","J_previous: 0.055975 \tJ_current: 0.055922 \tError: 0.095286\n","J_previous: 0.055922 \tJ_current: 0.055875 \tError: 0.085141\n","J_previous: 0.055875 \tJ_current: 0.055832 \tError: 0.076547\n","J_previous: 0.055832 \tJ_current: 0.055793 \tError: 0.069214\n","J_previous: 0.055793 \tJ_current: 0.055758 \tError: 0.062910\n","J_previous: 0.055758 \tJ_current: 0.055726 \tError: 0.057457\n","J_previous: 0.055726 \tJ_current: 0.055697 \tError: 0.052706\n","J_previous: 0.055697 \tJ_current: 0.055670 \tError: 0.048543\n","J_previous: 0.055670 \tJ_current: 0.055645 \tError: 0.044873\n","J_previous: 0.055645 \tJ_current: 0.055622 \tError: 0.041617\n","J_previous: 0.055622 \tJ_current: 0.055600 \tError: 0.038714\n","J_previous: 0.055600 \tJ_current: 0.055580 \tError: 0.036113\n","J_previous: 0.055580 \tJ_current: 0.055561 \tError: 0.033768\n","J_previous: 0.055561 \tJ_current: 0.055544 \tError: 0.031647\n","J_previous: 0.055544 \tJ_current: 0.055527 \tError: 0.029720\n","J_previous: 0.055527 \tJ_current: 0.055512 \tError: 0.027964\n","J_previous: 0.055512 \tJ_current: 0.055497 \tError: 0.026354\n","J_previous: 0.055497 \tJ_current: 0.055483 \tError: 0.024873\n","J_previous: 0.055483 \tJ_current: 0.055470 \tError: 0.023512\n","J_previous: 0.055470 \tJ_current: 0.055458 \tError: 0.022251\n","J_previous: 0.055458 \tJ_current: 0.055446 \tError: 0.021082\n","J_previous: 0.055446 \tJ_current: 0.055435 \tError: 0.019999\n","J_previous: 0.055435 \tJ_current: 0.055425 \tError: 0.018988\n","J_previous: 0.055425 \tJ_current: 0.055415 \tError: 0.018042\n","J_previous: 0.055415 \tJ_current: 0.055405 \tError: 0.017162\n","J_previous: 0.055405 \tJ_current: 0.055396 \tError: 0.016333\n","J_previous: 0.055396 \tJ_current: 0.055387 \tError: 0.015559\n","J_previous: 0.055387 \tJ_current: 0.055379 \tError: 0.014830\n","J_previous: 0.055379 \tJ_current: 0.055371 \tError: 0.014142\n","J_previous: 0.055371 \tJ_current: 0.055364 \tError: 0.013498\n","J_previous: 0.055364 \tJ_current: 0.055357 \tError: 0.012889\n","J_previous: 0.055357 \tJ_current: 0.055350 \tError: 0.012309\n","J_previous: 0.055350 \tJ_current: 0.055343 \tError: 0.011767\n","J_previous: 0.055343 \tJ_current: 0.055337 \tError: 0.011253\n","J_previous: 0.055337 \tJ_current: 0.055331 \tError: 0.010763\n","J_previous: 0.055331 \tJ_current: 0.055325 \tError: 0.010303\n","J_previous: 0.055325 \tJ_current: 0.055320 \tError: 0.009862\n"]},{"data":{"text/plain":["'\\n A continuación se despliegan para cada iteración los siguientes 3 elementos:\\n- El valor anterior de J \\n- El valor actual de J de esa iteración\\n- El error porcentual\\n'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["model = NeuronModel(X=X_train, y=y_train, learning_rate=10, error_threshold=0.01)\n","model.train(verbose=1)\n","\n","\"\"\"\n"," A continuación se despliegan para cada iteración los siguientes 3 elementos:\n","- El valor anterior de J \n","- El valor actual de J de esa iteración\n","- El error porcentual\n","\"\"\""]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":452},"executionInfo":{"elapsed":785,"status":"ok","timestamp":1726229560744,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"SW9hZDqqQisx","outputId":"aaa5c50f-e86b-47cc-8fdd-f061b509ae67"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["model.plot_cost_function()\n","\n","# Aquí se nos muestra como con cada iteración que se hace, nuestra función de costo \n","# se va actualizando y disminuyendo hasta alcanzar un valor mínimo estable"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":289,"status":"ok","timestamp":1726229564958,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"Xi1dSaZBQisx"},"outputs":[],"source":["y_predictions = model.predict(X=X_test)\n","\n","# Realizamos las predicciones de nuestro modelo usando los datos de prueba"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":424,"status":"ok","timestamp":1726229566800,"user":{"displayName":"Andrés Hernández Gutiérrez","userId":"09168378548171975941"},"user_tz":-600},"id":"gr9mNSCzQisx","outputId":"6d3a869d-9344-4cf6-9629-e0c073658b7f"},"outputs":[{"data":{"text/plain":["array([[-0.83457605],\n","       [ 0.24280479],\n","       [ 4.68126366],\n","       [ 1.05537486],\n","       [ 0.17704192],\n","       [-4.14819094]])"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["model.get_parameters()\n","\n","# Imprimimos los pesos y sesgo que obtuvo nuestro modelo después de ser entrenado\n","# Las primeras 5 filas son los pesos de nuestras entradas y la última fila es el sesgo"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"A22D2s2PQisx","outputId":"60daca34-5f6a-429b-f0f3-9e20250f2af3"},"outputs":[{"data":{"text/plain":["0.9897859922178989"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["model.evaluate(y_hat=y_predictions, y=y_test)\n","\n","# Evaluamos las predicciones de nuestro modelo de acuerdo a los datos verdaderos de nuestra salida\n","# o en otras palabras, el \"Accuracy\""]},{"cell_type":"markdown","metadata":{},"source":["### References\n","\n","* El libro de Python. (s.f.). Assert en Python. Obtenido de El Libro De Python: https://ellibrodepython.com/assert-python\n","* InteractiveChaos. (13 de Enero de 2021). isinstance. Obtenido de InteractiveChaos: https://interactivechaos.com/es/python/function/isinstance\n","* Khetarpal, A. (s.f.). numpy.vstack. Obtenido de NumPy: https://numpy.org/doc/stable/reference/generated/numpy.vstack.html\n","* Matthias. (3 de Noviembre de 2018). Meaning of %8.2f , %2d etc... Obtenido de Sololearn: https://www.sololearn.com/en/Discuss/1572079/meaning-of-82f-2d-etc\n","* Ram, V. (3 de Agosto de 2022). How to use Python numpy.where() Method. Obtenido de DigitalOcean: https://www.digitalocean.com/community/tutorials/python-numpy-where\n","* The SciPy community. (s.f.). scipy.special.expit. Obtenido de SciPy: https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.expit.html\n","* W3Schools. (s.f.). NumPy Array Reshaping. Obtenido de W3Schools: https://www.w3schools.com/python/numpy/numpy_array_reshape.asp\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"ai","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":0}
